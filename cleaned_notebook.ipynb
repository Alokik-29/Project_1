{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9zBM8yL_zqS",
    "outputId": "ca7746eb-b7e3-48b1-a095-7bd0365596be"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os, sys, random, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa, librosa.display\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from transformers import ASTForAudioClassification, ASTFeatureExtractor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verbosity control\n",
    "VERBOSE = False\n",
    "def printv(*args, **kwargs):\n",
    "    if VERBOSE:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Librosa: {librosa.__version__} | Torchaudio: {torchaudio.__version__}\")\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"axes.grid\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Fa2BMdR_0sf",
    "outputId": "a58b3f33-1297-4792-f371-74bf3c7cbde0"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42  # for reproducibility\n",
    "\n",
    "emotion_map = {\n",
    "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
    "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
    "}\n",
    "valid_labels = list(emotion_map.values())\n",
    "\n",
    "# Download RAVDESS dataset\n",
    "dataset_path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "print(\"RAVDESS downloaded.\")\n",
    "\n",
    "# Collect files and labels\n",
    "audio_files = list(Path(dataset_path).rglob(\"*.wav\"))\n",
    "labels = []\n",
    "for file in audio_files:\n",
    "    try:\n",
    "        emotion_code = file.name.split(\"-\")[2]\n",
    "        labels.append(emotion_map.get(emotion_code))\n",
    "    except Exception:\n",
    "        labels.append(None)\n",
    "\n",
    "df = pd.DataFrame({\"file_path\": audio_files, \"label\": labels})\n",
    "df = df[df['label'].isin(valid_labels)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"Class distribution:\\n\", df['label'].value_counts())\n",
    "df = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "# Load CREMA-D Dataset\n",
    "print(\"\\nLoading CREMA-D dataset...\")\n",
    "crema_path = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
    "\n",
    "crema_emotion_map = {\n",
    "    \"NEU\": \"neutral\", \"ANG\": \"angry\", \"DIS\": \"disgust\",\n",
    "    \"FEA\": \"fearful\", \"HAP\": \"happy\", \"SAD\": \"sad\"\n",
    "}\n",
    "\n",
    "crema_files = list(Path(crema_path).rglob(\"*.wav\"))\n",
    "crema_labels = []\n",
    "for f in crema_files:\n",
    "    code = f.name.split(\"_\")[2] if \"_\" in f.name else \"\"\n",
    "    crema_labels.append(crema_emotion_map.get(code))\n",
    "\n",
    "crema_df = pd.DataFrame({\"file_path\": crema_files, \"label\": crema_labels}).dropna().reset_index(drop=True)\n",
    "crema_df = crema_df[crema_df['label'].isin(emotion_map.values())].reset_index(drop=True)\n",
    "\n",
    "print(f\"CREMA-D samples: {len(crema_df)}\")\n",
    "print(\"CREMA-D class distribution:\\n\", crema_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "WSyeuc1K_63v",
    "outputId": "a4caf507-b70a-4281-f6ec-c53c870e3907"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Data Visualization\n",
    "def visualize_dataset_distribution():\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['label'].value_counts().plot(kind='bar')\n",
    "    plt.title('RAVDESS Dataset - Emotion Distribution')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if not df.empty:\n",
    "        sample_file = df['file_path'].iloc[0]\n",
    "        sample_label = df['label'].iloc[0]\n",
    "        y, sr = librosa.load(sample_file, sr=16000)\n",
    "        librosa.display.waveshow(y, sr=sr)\n",
    "        plt.title(f\"Sample Waveform - {sample_label}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_dataset_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Z_OP2kfABFJ"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Audio Augmentation Functions\n",
    "def augment_audio(y, sr=16000):\n",
    "    \"\"\"Simple audio augmentation techniques\"\"\"\n",
    "    y_aug = y.copy()\n",
    "\n",
    "    # Random noise addition\n",
    "    if random.random() > 0.5:\n",
    "        noise_factor = 0.005\n",
    "        noise = np.random.normal(0, noise_factor, y_aug.shape)\n",
    "        y_aug = y_aug + noise\n",
    "\n",
    "    # Time shifting\n",
    "    if random.random() > 0.5:\n",
    "        shift_max = sr // 4  # Max shift of 0.25 seconds\n",
    "        shift = random.randint(-shift_max, shift_max)\n",
    "        y_aug = np.roll(y_aug, shift)\n",
    "\n",
    "    # Volume scaling\n",
    "    if random.random() > 0.5:\n",
    "        scale_factor = random.uniform(0.8, 1.2)\n",
    "        y_aug = y_aug * scale_factor\n",
    "\n",
    "    return y_aug.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4BFs2CIEJOp"
   },
   "outputs": [],
   "source": [
    "#Cell 5: Improved Dataset Classes with Augmentation\n",
    "class ImprovedDataset(Dataset):\n",
    "    def __init__(self, df, target_sr=16000, max_length=None, augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_sr = target_sr\n",
    "        self.label_list = list(emotion_map.values())\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = str(self.df.iloc[idx]['file_path'])\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        if label not in self.label_list:\n",
    "            raise ValueError(f\"Invalid label found: {label}\")\n",
    "\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=self.target_sr)\n",
    "            y = y.astype(np.float32)\n",
    "\n",
    "            # Apply augmentation during training\n",
    "            if self.augment:\n",
    "                y = augment_audio(y, sr=self.target_sr)\n",
    "\n",
    "            if self.max_length and len(y) > self.max_length:\n",
    "                y = y[:self.max_length]\n",
    "\n",
    "        except Exception as e:\n",
    "            printv(f\"Error loading {file_path}: {e}\")\n",
    "            y = np.zeros(self.target_sr, dtype=np.float32)\n",
    "\n",
    "        label_idx = self.label_list.index(label)\n",
    "        return y, label_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_MMGCI_AFZZ"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Improved Collate Function\n",
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    waveforms = [torch.tensor(w) if not isinstance(w, torch.Tensor) else w for w in waveforms]\n",
    "    max_len = max(w.shape[0] for w in waveforms)\n",
    "    padded_waveforms = []\n",
    "    for w in waveforms:\n",
    "        if w.shape[0] < max_len:\n",
    "            padded = torch.nn.functional.pad(w, (0, max_len - w.shape[0]))\n",
    "        else:\n",
    "            padded = w\n",
    "        padded_waveforms.append(padded)\n",
    "    waveforms_tensor = torch.stack(padded_waveforms)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return waveforms_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMlnctk3AJGN",
    "outputId": "eb9d009b-9e93-4816-b93f-bd351011bd64"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Combined Dataset Preparation\n",
    "# Mix datasets for better generalization\n",
    "print(\"Creating combined training dataset...\")\n",
    "\n",
    "# Take 20% of CREMA-D samples for training\n",
    "crema_train_sample = crema_df.sample(frac=0.1, random_state=SEED).reset_index(drop=True)\n",
    "print(f\"Using {len(crema_train_sample)} CREMA-D samples for training\")\n",
    "\n",
    "# Split RAVDESS\n",
    "ravdess_train, ravdess_test = train_test_split(\n",
    "    df, test_size=0.2, random_state=SEED, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Combine RAVDESS training with CREMA-D sample\n",
    "combined_train_df = pd.concat([ravdess_train, crema_train_sample]).reset_index(drop=True)\n",
    "\n",
    "# Use remaining CREMA-D for validation\n",
    "crema_val_df = crema_df.drop(crema_train_sample.index).reset_index(drop=True)\n",
    "\n",
    "print(f\"Combined training samples: {len(combined_train_df)}\")\n",
    "print(f\"RAVDESS test samples: {len(ravdess_test)}\")\n",
    "print(f\"CREMA-D validation samples: {len(crema_val_df)}\")\n",
    "\n",
    "# Create datasets with augmentation for training\n",
    "train_dataset = ImprovedDataset(combined_train_df, augment=True)\n",
    "test_dataset = ImprovedDataset(ravdess_test, augment=False)\n",
    "val_dataset = ImprovedDataset(crema_val_df, augment=False)\n",
    "\n",
    "# Data loaders (tuned for speed + stability)\n",
    "BATCH_SIZE = 16  # keep as you set; lower if you hit OOM\n",
    "COMMON_KW = dict(\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,              # increase to 4 if your env supports it\n",
    "    pin_memory=True,            # faster host->GPU transfers\n",
    "    persistent_workers=True,    # avoid worker re-spawns\n",
    "    prefetch_factor=2,          # small but helpful\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  **COMMON_KW)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, **COMMON_KW)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, **COMMON_KW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "d60d01415799407eaff1a1a35a3b0809",
      "40aedf76e2d74f95b71af272b90f3afe",
      "80104bed261c4941b9872ee892eccb6a",
      "942c3f0ef26147e3b4354b30bf8c8c25",
      "9530995e5a7f4fdbbd38190071c419a2",
      "5393db7499104113bcb72d44b601ab67",
      "b335aa352e494a8d81fd3c5ba75b9489",
      "02a120e0b5f343c68ca8f89f9dabfe5b",
      "feec2c442f214ebe96371fb00502be16",
      "7b5b8178e2034b70928b7f80e2b5f4fa",
      "2b913428fac0493e881bc0c5bce71120"
     ]
    },
    "id": "IowiPXuEAVC9",
    "outputId": "fe11cecf-596b-4ada-8a8c-912860ff0e3e"
   },
   "outputs": [],
   "source": [
    "# Cell 8: Improved Model Setup\n",
    "def setup_improved_model():\n",
    "    \"\"\"Setup model with simpler classifier to prevent overfitting\"\"\"\n",
    "    feature_extractor = ASTFeatureExtractor.from_pretrained(\n",
    "        \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    )\n",
    "    model = ASTForAudioClassification.from_pretrained(\n",
    "        \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "    )\n",
    "\n",
    "    num_labels = len(emotion_map)\n",
    "    # Simpler classifier to reduce overfitting\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.5),  # Higher dropout\n",
    "        nn.Linear(model.config.hidden_size, num_labels)\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    return model, feature_extractor\n",
    "\n",
    "model, feature_extractor = setup_improved_model()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,} | Trainable: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AakroeaYAX5x"
   },
   "outputs": [],
   "source": [
    "# Cell 9: Improved Training Function with Early Stopping (fast + AMP)\n",
    "\n",
    "def _batch_to_inputs(waveforms):\n",
    "    \"\"\"Run feature_extractor once per batch (much faster).\"\"\"\n",
    "    with torch.no_grad():  # extractor is non-trainable\n",
    "        feats = feature_extractor(\n",
    "            [w.cpu().numpy() for w in waveforms],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )[\"input_values\"]\n",
    "    return feats\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, scaler=None):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for waveforms, labels in train_loader:\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        inputs = _batch_to_inputs(waveforms).to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(inputs).logits\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(inputs).logits\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = logits.argmax(1)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "        # free intermediates ASAP\n",
    "        del inputs, logits, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_simple(model, data_loader):\n",
    "    \"\"\"Fast eval with batch feature extraction + no grads.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for waveforms, labels in data_loader:\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        inputs = _batch_to_inputs(waveforms).to(device, non_blocking=True)\n",
    "        logits = model(inputs).logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        del inputs, logits, preds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    return accuracy\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, val_loader, num_epochs=10, lr=5e-5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, scaler)\n",
    "\n",
    "        # Validate every 2 epochs (0-based: 0,2,4,...) and on last epoch\n",
    "        do_val = (epoch % 2 == 0) or (epoch == num_epochs - 1)\n",
    "        val_acc = evaluate_model_simple(model, val_loader) if do_val else best_val_acc\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs} | Loss: {train_loss:.4f} | \"\n",
    "              f\"Train: {train_acc:.2f}% | Val: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_improved_model.pth\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping after {patience} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "    return history, best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "eR-c9aQ9AkkH",
    "outputId": "3cf79def-0697-4140-e2c4-d5bf1fdaf69b"
   },
   "outputs": [],
   "source": [
    "# Cell 10: Train the Improved Model (fixed call + best_val_acc)\n",
    "print(\"Starting improved training...\")\n",
    "history, best_val_acc = train_with_early_stopping(\n",
    "    model, train_loader, val_loader,\n",
    "    num_epochs=15, lr=5e-5\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "train_losses = history[\"train_loss\"]\n",
    "train_accs   = history[\"train_acc\"]\n",
    "val_accs     = history[\"val_acc\"]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accs, label='Training')\n",
    "plt.plot(val_accs, label='Validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "overfitting_gap = np.array(train_accs) - np.array(val_accs)\n",
    "plt.plot(overfitting_gap)\n",
    "plt.title('Overfitting Monitor (Train - Val)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc Diff (%)')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training completed. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n9SC86DArEj"
   },
   "outputs": [],
   "source": [
    "# Cell 11: Detailed Evaluation Function (fast)\n",
    "import torch\n",
    "@torch.no_grad()\n",
    "def evaluate_model_detailed(model, data_loader, dataset_name, verbose=True):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for waveforms, labels in data_loader:\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        inputs = feature_extractor(\n",
    "            [w.cpu().numpy() for w in waveforms],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )[\"input_values\"].to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(inputs).logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        del inputs, logits, preds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n{dataset_name} Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "        label_names = list(emotion_map.values())\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=label_names))\n",
    "\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=label_names, yticklabels=label_names)\n",
    "        plt.title(f'{dataset_name} - Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373,
     "referenced_widgets": [
      "80bf6c69bd3e46099e732b1748808454",
      "6db25e900f664580835d012882d778a6",
      "5cd3a4749f2c41c9b1fc7762b9370d1a",
      "c919203501d04551990768de94be8e00",
      "dcb9bd050e1f4183bb62d25c1790c3a9",
      "b97834ba0b7a49f7a6bbe0ef496d7c03",
      "03524d8d74b84b35b8a6e1745ce96c90",
      "85dee5efb430498480c0a5f59ff31c7a",
      "11487ecfa996451eb2a67b9f694813b9",
      "3b79ba975fe940138ee48553f3761222",
      "fd278fa4760c403e980855931648a160",
      "d1079c71de88472f8c8b94eded0a4f18",
      "fec26bb13e614dd48637fed5d4d6bb42",
      "ec6b60f2443f438289e0cbce60cc7017",
      "c3e8a9bfb52749e9a23caa40e6ad62b0",
      "902bbd9e45d74277903f6f5f09bcdbcf",
      "9b59d5e3826444a1bddedf4fbb041480",
      "31457e8fea3d4e62b7858ba9a7b55c56",
      "2beaaef39d06458ab56f5ae6d27c9599",
      "c368195c2a8f45599b437516dc858615",
      "b2a7acd024f546a292e74e0114cb1273",
      "767c18b8713f4607868e580e36174dff"
     ]
    },
    "id": "s_ooIBAoAt95",
    "outputId": "8590d173-306d-47ed-f7ca-a1a8824314d8"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 📌 Cell 12: Model Evaluation & Report\n",
    "# ================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# 1. Emotion mapping (must match training setup)\n",
    "emotion_map = {\n",
    "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
    "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
    "}\n",
    "num_classes = len(emotion_map)  # = 8\n",
    "\n",
    "# 2. Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device → {device}\")\n",
    "\n",
    "# 3. Rebuild the AST model class (must match training definition)\n",
    "class ASTModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(ASTModel, self).__init__()\n",
    "        from transformers import ASTModel as PretrainedAST, ASTConfig\n",
    "        config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast = PretrainedAST.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=config)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.fc = torch.nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.ast(input_values).last_hidden_state\n",
    "        pooled = outputs.mean(dim=1)  # mean pooling\n",
    "        x = self.dropout(pooled)\n",
    "        return self.fc(x)\n",
    "\n",
    "# 4. Initialize model and load checkpoint\n",
    "model = ASTModel(num_classes=num_classes).to(device)\n",
    "\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "try:\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(f\"✅ Loaded model weights from {checkpoint_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Warning: No saved model checkpoint found, using randomly initialized model.\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 5. Generate final report\n",
    "print(\"=\"*60)\n",
    "print(\"📊 EMOTION RECOGNITION MODEL - FINAL REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✔️ Model Architecture: AST (Audio Spectrogram Transformer)\")\n",
    "print(f\"✔️ Number of Classes: {num_classes} ({list(emotion_map.values())})\")\n",
    "print(f\"✔️ Device Used: {device}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXtx_tfHAwf1"
   },
   "outputs": [],
   "source": [
    "# Cell 13: Sample Predictions Analysis (faster)\n",
    "@torch.no_grad()\n",
    "def analyze_predictions(model, data_loader, dataset_name, num_samples=8):\n",
    "    model.eval()\n",
    "    samples_analyzed = 0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    print(f\"\\n{dataset_name} - Sample Predictions:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for waveforms, labels in data_loader:\n",
    "        if samples_analyzed >= num_samples:\n",
    "            break\n",
    "\n",
    "        labels = labels.to(device)\n",
    "        take = min(waveforms.shape[0], num_samples - samples_analyzed)\n",
    "\n",
    "        inputs = feature_extractor(\n",
    "            [w.cpu().numpy() for w in waveforms[:take]],\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )[\"input_values\"].to(device)\n",
    "\n",
    "        logits = model(inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        for i in range(take):\n",
    "            true_label = list(emotion_map.values())[labels[i]]\n",
    "            pred_label = list(emotion_map.values())[preds[i]]\n",
    "            confidence = probs[i][preds[i]].item()\n",
    "\n",
    "            is_correct = (true_label == pred_label)\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            status = \"✓\" if is_correct else \"✗\"\n",
    "            print(f\"#{samples_analyzed + 1:2d} True: {true_label:10} | Pred: {pred_label:10} | Conf: {confidence:.3f} | {status}\")\n",
    "            samples_analyzed += 1\n",
    "\n",
    "        del inputs, logits, probs, preds\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    sample_accuracy = (correct_predictions / max(samples_analyzed,1)) * 100\n",
    "    print(f\"\\nSample accuracy: {correct_predictions}/{samples_analyzed} ({sample_accuracy:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSR7RFC8A0NO",
    "outputId": "9c4da74b-d2fc-4621-c520-5204e3a44ea7"
   },
   "outputs": [],
   "source": [
    "# Cell 14: Deployment Class\n",
    "class ImprovedEmotionPredictor:\n",
    "    def __init__(self, model_path, feature_extractor):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.emotion_labels = list(emotion_map.values())\n",
    "\n",
    "        # Initialize model\n",
    "        self.model = ASTForAudioClassification.from_pretrained(\n",
    "            \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "        )\n",
    "        num_labels = len(emotion_map)\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.model.config.hidden_size, num_labels)\n",
    "        )\n",
    "\n",
    "        # Load trained weights\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict_emotion(self, audio_path_or_array, return_probabilities=False):\n",
    "        \"\"\"Predict emotion from audio file or numpy array\"\"\"\n",
    "        if isinstance(audio_path_or_array, (str, Path)):\n",
    "            y, sr = librosa.load(audio_path_or_array, sr=16000)\n",
    "        else:\n",
    "            y = audio_path_or_array\n",
    "\n",
    "        y = y.astype(np.float32)\n",
    "        inputs = self.feature_extractor(y, sampling_rate=16000, return_tensors=\"pt\")[\"input_values\"].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs).logits\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            predicted_class = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        predicted_emotion = self.emotion_labels[predicted_class[0]]\n",
    "        confidence = probabilities[0][predicted_class[0]].item()\n",
    "\n",
    "        if return_probabilities:\n",
    "            all_probs = {label: prob.item() for label, prob in zip(self.emotion_labels, probabilities[0])}\n",
    "            return predicted_emotion, confidence, all_probs\n",
    "\n",
    "        return predicted_emotion, confidence\n",
    "\n",
    "    def predict_batch(self, audio_files):\n",
    "        \"\"\"Predict emotions for multiple audio files\"\"\"\n",
    "        results = []\n",
    "        for audio_file in audio_files:\n",
    "            try:\n",
    "                emotion, confidence = self.predict_emotion(audio_file)\n",
    "                results.append({\n",
    "                    'file': audio_file,\n",
    "                    'predicted_emotion': emotion,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'file': audio_file,\n",
    "                    'predicted_emotion': 'error',\n",
    "                    'confidence': 0.0,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        return results\n",
    "\n",
    "# Initialize improved predictor\n",
    "if best_model_path.exists():\n",
    "    predictor = ImprovedEmotionPredictor('best_improved_model.pth', feature_extractor)\n",
    "    print(\"✅ Improved emotion predictor ready for deployment!\")\n",
    "else:\n",
    "    print(\"⚠️ Model file not found. Please train the model first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ippmqwE6A0ox",
    "outputId": "f0379b78-4372-4166-c35d-df627aa8f10e"
   },
   "outputs": [],
   "source": [
    "# Cell 15: Final Report\n",
    "def generate_improved_model_report():\n",
    "    print(\"=\"*70)\n",
    "    print(\"IMPROVED EMOTION RECOGNITION MODEL - FINAL REPORT\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"\\n🔧 IMPROVEMENTS MADE:\")\n",
    "    print(\"  • Added audio augmentation (noise, time shift, volume)\")\n",
    "    print(\"  • Mixed RAVDESS + CREMA-D for training\")\n",
    "    print(\"  • Reduced learning rate (1e-4 → 5e-5)\")\n",
    "    print(\"  • Simplified classifier architecture\")\n",
    "    print(\"  • Added early stopping (patience=5)\")\n",
    "    print(\"  • Added gradient clipping\")\n",
    "    print(\"  • Increased dropout (0.3 → 0.5)\")\n",
    "\n",
    "    print(\"\\n📋 MODEL ARCHITECTURE:\")\n",
    "    print(\"  • Base Model: Audio Spectrogram Transformer (AST)\")\n",
    "    print(\"  • Pre-trained on: AudioSet\")\n",
    "    print(f\"  • Fine-tuned for: {len(emotion_map)} emotion classes\")\n",
    "    print(f\"  • Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  • Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    print(\"\\n📊 TRAINING DATA:\")\n",
    "    print(f\"  • RAVDESS Training: {len(combined_train_df)} samples\")\n",
    "    print(f\"  • RAVDESS Test: {len(ravdess_test)} samples\")\n",
    "    print(f\"  • CREMA-D Validation: {len(crema_val_df)} samples\")\n",
    "    print(f\"  • Emotion Classes: {', '.join(emotion_map.values())}\")\n",
    "\n",
    "    print(\"\\n🏆 PERFORMANCE RESULTS:\")\n",
    "    if 'ravdess_acc' in globals():\n",
    "        print(f\"  • RAVDESS Test Accuracy: {ravdess_acc:.2f}%\")\n",
    "    if 'crema_acc' in globals():\n",
    "        print(f\"  • CREMA-D Validation Accuracy: {crema_acc:.2f}%\")\n",
    "        print(f\"  • Generalization Gap: {ravdess_acc - crema_acc:.2f}%\")\n",
    "\n",
    "    print(\"\\n⚡ KEY FEATURES:\")\n",
    "    print(\"  • Cross-dataset training for better generalization\")\n",
    "    print(\"  • Audio augmentation for robustness\")\n",
    "    print(\"  • Early stopping to prevent overfitting\")\n",
    "    print(\"  • Deployment-ready predictor class\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Generate the final report\n",
    "generate_improved_model_report()\n",
    "\n",
    "print(\"\\n🎉 IMPROVED MODEL TRAINING COMPLETE!\")\n",
    "print(\"Your model is now ready for deployment and should show better generalization!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fp5l_xe2ynVz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
